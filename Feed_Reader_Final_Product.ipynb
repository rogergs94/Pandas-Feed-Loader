{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In order to make it work, you’ll need to either click “**Run All**” or run each cell one by one from the beginning.\n",
        "\n",
        "This code loads structured data from a URL and converts it into a Pandas DataFrame.\n",
        "It supports two formats: JSON and XML (including .xml.gz files compressed with GZIP).\n",
        "\n",
        "**1. Format detection**\n",
        "\n",
        "  - If the HTTP response header contains Content-Type: application/json, or if the URL ends with .json or contains \"rest-api\", the content is treated as JSON.\n",
        "\n",
        "  - Otherwise, the content is treated as XML.\n",
        "\n",
        "**2. JSON processing**\n",
        "\n",
        " - Can handle:\n",
        "\n",
        "    + A list of objects ([ {...}, {...} ])\n",
        "\n",
        "    + A dictionary containing a \"jobs\" key ({\"jobs\": [ {...}, {...} ]})\n",
        "\n",
        "    + Any other JSON object that can be converted into a DataFrame.\n",
        "\n",
        "**3. XML processing**\n",
        "\n",
        "- Works with both standard .xml files and .xml.gz (GZIP compressed).\n",
        "\n",
        "- Searches for elements matching the job_tag parameter (default: \"job\") and extracts their child tags and values.\n",
        "\n",
        "4. Data cleaning **bold text**\n",
        "\n",
        "- Truncates overly long text values.\n",
        "\n",
        "- Replaces invalid numbers (NaN or infinite values) with NaN.\n",
        "\n",
        "- Removes columns that are entirely empty, or fills missing values with 0.\n",
        "\n",
        "**In short, load_feed_to_dataframe() automatically detects the data format, reads it, cleans it, and returns it as a ready-to-use Pandas DataFrame for analysis.**\n"
      ],
      "metadata": {
        "id": "hDCK9rWgi9-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpGwyfGCXItO"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "##JSON FUNCTION\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "import gzip\n",
        "import datetime\n",
        "\n",
        "# --- Aux functions ---\n",
        "\n",
        "def truncate(value, max_length=49000):\n",
        "    if value and isinstance(value, str) and len(value) > max_length:\n",
        "        return value[:max_length]\n",
        "    return value\n",
        "\n",
        "def clean_invalid_numbers(df):\n",
        "    return df.apply(lambda col: col.map(lambda x: np.nan if isinstance(x, float) and (np.isnan(x) or np.isinf(x)) else x))\n",
        "\n",
        "\n",
        "def import_datetime(df):\n",
        "    df['last_update'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "def drop_all_nulls(df):\n",
        "    for column in df.columns:\n",
        "        if df[column].isnull().all():\n",
        "            df.drop(columns=[column], inplace=True)\n",
        "            print(f\"Column '{column}' has been dropped due to all NaN values.\")\n",
        "        else:\n",
        "            df[column].fillna(0, inplace=True)\n",
        "\n",
        "# --- PANDAS. Main function to load the XML ---\n",
        "\n",
        "def load_feed_to_dataframe(url, job_tag=\"job\"):\n",
        "    \"\"\"\n",
        "    Loads an XML feed (.xml or .xml.gz) or JSON from a URL and converts it into a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the feed.\n",
        "        job_tag (str): Name of the XML tag representing each job (only for XML feeds).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the feed data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Try to parse as JSON if the content-type indicates it or the URL suggests .json\n",
        "        if \"application/json\" in response.headers.get(\"Content-Type\", \"\") or url.endswith(\".json\") or \"rest-api\" in url:\n",
        "            data = response.json()\n",
        "\n",
        "            # Handle different JSON formats\n",
        "            if isinstance(data, list):\n",
        "                df = pd.DataFrame(data)\n",
        "            elif isinstance(data, dict) and \"jobs\" in data:\n",
        "                df = pd.DataFrame(data[\"jobs\"])\n",
        "            else:\n",
        "                df = pd.DataFrame(data)\n",
        "\n",
        "            # Truncate and clean\n",
        "            df = df.applymap(lambda x: truncate(x) if isinstance(x, str) else x)\n",
        "            df = clean_invalid_numbers(df)\n",
        "            return df\n",
        "\n",
        "        # If not JSON, treat as XML\n",
        "        if url.endswith(\".gz\"):\n",
        "            with gzip.GzipFile(fileobj=BytesIO(response.content)) as f:\n",
        "                xml_content = f.read()\n",
        "        else:\n",
        "            xml_content = response.content\n",
        "\n",
        "        root = ET.fromstring(xml_content)\n",
        "        items = root.findall(f\".//{job_tag}\")\n",
        "        if not items:\n",
        "            print(f\"No <{job_tag}> elements found in the XML.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        jobs_data = []\n",
        "        for job in items:\n",
        "            job_data = {child.tag: truncate(child.text) for child in job}\n",
        "            jobs_data.append(job_data)\n",
        "\n",
        "        df = pd.DataFrame(jobs_data)\n",
        "        df = clean_invalid_numbers(df)\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the feed: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "feed_url = input(\"Feed URL?  \")\n",
        "\n",
        "df = load_feed_to_dataframe(feed_url)\n",
        "\n",
        "\n",
        "# =====================================================================================\n",
        "#Change NaN with 0\n",
        "print(f\"NUMBER OF ROWS: {df.shape[0]}\")\n",
        "print(f\"NUMBER OF COLUMNS: {df.shape[1]}\")\n",
        "print(\"NAN VALUES:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "#Transform all the Nan values into 0.\n",
        "df = df.fillna(0)\n",
        "\n",
        "'''\n",
        "for column in df.columns:\n",
        "    if df[column].isnull().all():\n",
        "        df = df.drop(columns=[column])\n",
        "        print(f\"Column '{column}' has been dropped due to all NaN values.\")\n",
        "    else:\n",
        "      print(f\"Column '{column}' has values.\")\n",
        "    '''\n",
        "\n",
        "# =====================================================================================\n",
        "df.head() #it shows the df with the first 5 rows\n"
      ],
      "metadata": {
        "id": "I7Qk2tSFXTNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "7360AKTIoIGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# Run this cell to download a .csv file with feed\n",
        "\n",
        "df.to_csv('feed.csv', index=False)\n",
        "from google.colab import files\n",
        "files.download('feed.csv')\n"
      ],
      "metadata": {
        "id": "whUegIbfcX6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# Write the name of the CLIENT in the opt2 that you want to downlowad (ie:Enhance). It must have this structure: 'xxxxx'\n",
        "\n",
        "opt2 = 'FOruntosd'\n",
        "\n",
        "# Keep in mind that you will be reading all the rows that match the value in opt2 in the column client (df.client). If you want to read another column, just change client for name of the columna that you want to filter\n",
        "company = df[df.clientname.str.contains(opt2, na=False)]\n",
        "company.head()\n",
        "\n",
        "# Run this cell to download a .csv file with the CLIENT given\n",
        "\n",
        "company.to_csv(f'{opt2}.csv', index=False)\n",
        "from google.colab import files\n",
        "files.download(f'{opt2}.csv')\n"
      ],
      "metadata": {
        "id": "ZdYEbOiA2KC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6QTXgxPpkTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}